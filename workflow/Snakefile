import pandas as pd
import os
from snakemake.utils import validate

validate(config, "schemas/config.schema.yml")


include: "scripts/common.py"
include: "rules/mkcoinr.smk"


localrules:
    parse_results,
    multiqc,
    parse_sintax,
    get_reads,
    map_qc,
    download_taxdump,
    collate_sintax,
    parse_genome_counts,
    samtools_stats,
    krona,
    all,


# Read sample list
sample_df = pd.read_csv(
    config["sample_list"], comment="#", index_col=0, dtype={"sample_name": str}
)
sample_df.rename(
    columns={"fwd": "fwd_libs", "rev": "rev_libs", "type": "lib_type"}, inplace=True
)
samples = sample_df.to_dict(orient="index")

# Read mapping list
mapping_df = pd.read_csv(config["mappings_list"], index_col=0)
mappings = mapping_df.to_dict(orient="index")


wildcard_constraints:
    results_dir="\w+",
    sample=f"({'|'.join(list(samples.keys()))})",


## TARGETS ##
# These rules can be targeted with snakemake on the command line:
# e.g. snakemake ... genome_map sintax ...


rule all:
    """Workflow pseudo rule"""
    input:
        all_output,


def genome_map_input(wildcards):
    if "mappings" in config.keys():
        files = expand(
            "{results_dir}/genome_mappings/{ref}/bowtie2/{sample}/{sample}.reads.fa",
            results_dir=config["results_dir"],
            sample=samples_to_genomemap(config, samples),
            ref=config["mappings"]["genomes"].keys(),
        )
    else:
        files = []
    return files


rule genome_map:
    """
    Performs the mapping against genomes and extraction of reads from bamfiles
    for all samples
    """
    input:
        genome_map_input,


def genome_count_input(wildcards):
    if "mappings" in config.keys():
        files = expand(
            "{results_dir}/genome_mappings/{ref}/summary_{f}.csv",
            results_dir=config["results_dir"],
            f=["raw_counts", "size_adjusted"],
            t=["taxid", "species"],
            ref=config["mappings"]["genomes"].keys(),
        )
    else:
        files = []
    return files


rule genome_count:
    """
    Performs genome mapping + summarizing results into raw + size adjusted counts
    """
    input:
        genome_count_input,


rule coi_map:
    """
    Runs the COI mapping + filtering with krakenuniq + extraction of reads
    """
    input:
        expand(
            "{results_dir}/mappings/{map_name}/{mapper}/{sample}.krakenuniq.filtered.fastq.gz",
            results_dir=config["results_dir"],
            map_name=config["mappings"]["marker_genes"].keys(),
            mapper=config["mappers"],
            sample=samples.keys(),
        ),


rule sintax:
    """
    Generate sintax output for all samples
    """
    input:
        expand(
            "{results_dir}/mappings/{map_name}/{mapper}/krona/krona.html",
            results_dir=config["results_dir"],
            map_name=config["mappings"]["marker_genes"].keys(),
            mapper=config["mappers"],
        ),
        expand(
            "{results_dir}/mappings/counts/sintax.{map_name}.{mapper}.tsv",
            results_dir=config["results_dir"],
            map_name=config["mappings"]["marker_genes"].keys(),
            mapper=config["mappers"],
        ),


## PREPROCESSING ##


rule fastp:
    """
    Runs fastp preprocessing on the raw reads
    """
    output:
        R1=temp("{results_dir}/fastp/{sample}.fastp.R1.fastq.gz"),
        R2=temp("{results_dir}/fastp/{sample}.fastp.R2.fastq.gz"),
    log:
        shell="{results_dir}/logs/fastp/{sample}.fastp.shell.log",
        log="{results_dir}/logs/fastp/{sample}.fastp.log",
        json="{results_dir}/logs/fastp/{sample}.fastp.json",
    input:
        R1=lambda wildcards: sorted(
            [x for x in samples[wildcards.sample]["fwd_libs"].split(";")]
        ),
        R2=lambda wildcards: sorted(
            [x for x in samples[wildcards.sample]["rev_libs"].split(";")]
        ),
    envmodules:
        "bioinfo-tools",
        "fastp/0.23.2",
    conda:
        "envs/fastp.yml"
    group:
        "group1"
    resources:
        runtime=60,
        mem_mb=mem_allowed,
    threads: 20
    params:
        tmpR1="$TMPDIR/{sample}.R1.fastq.gz",
        tmpR2="$TMPDIR/{sample}.R2.fastq.gz",
        outR1="$TMPDIR/{sample}.fastp.R1.fastq.gz",
        outR2="$TMPDIR/{sample}.fastp.R2.fastq.gz",
        complexity_threshold=config["fastp"]["complexity_threshold"],
        min_length=config["fastp"]["min_length"],
        settings=config["fastp"]["settings"],
    shell:
        """
        exec &>{log.shell}
        cat {input.R1} > {params.tmpR1}
        cat {input.R2} > {params.tmpR2}
        fastp --thread {threads} -y -Y {params.complexity_threshold} -l {params.min_length} \
            -i {params.tmpR1} -I {params.tmpR2} -o {params.outR1} \
            -O {params.outR2} -j {log.json} {params.settings} > {log.log} 2>&1
        mv {params.outR1} {output.R1}
        mv {params.outR2} {output.R2}
        rm {params.tmpR1} {params.tmpR2}
        """


rule fastp_dedup:
    output:
        R1=temp("{results_dir}/fastp/{sample}.fastp.dedup.R1.fastq.gz"),
        R2=temp("{results_dir}/fastp/{sample}.fastp.dedup.R2.fastq.gz"),
    input:
        R1=rules.fastp.output.R1,
        R2=rules.fastp.output.R2,
    log:
        shell="{results_dir}/logs/fastp/{sample}.fastp.dedup.shell.log",
        log="{results_dir}/logs/fastp/{sample}.fastp.dedup.log",
        json="{results_dir}/logs/fastp/{sample}.fastp.dedup.json",
    params:
        tmpR1="$TMPDIR/{sample}.R1.fastq.gz",
        tmpR2="$TMPDIR/{sample}.R2.fastq.gz",
        outR1="$TMPDIR/{sample}.fastp.R1.fastq.gz",
        outR2="$TMPDIR/{sample}.fastp.R2.fastq.gz",
        complexity_threshold=config["fastp"]["complexity_threshold"],
    envmodules:
        "bioinfo-tools",
        "fastp/0.23.2",
    conda:
        "envs/fastp.yml"
    group:
        "group1"
    threads: 10
    resources:
        runtime=60,
        mem_mb=mem_allowed,
    shell:
        """
        exec &>{log.shell}
        cat {input.R1} > {params.tmpR1}
        cat {input.R2} > {params.tmpR2}
        fastp --thread {threads} -i {params.tmpR1} -I {params.tmpR2} -o {params.outR1} \
            -O {params.outR2} -j {log.json} --dedup --disable_adapter_trimming \
            --disable_quality_filtering --disable_trim_poly_g --disable_length_filtering > {log.log} 2>&1
        mv {params.outR1} {output.R1}
        mv {params.outR2} {output.R2}
        rm {params.tmpR1} {params.tmpR2}        
        """


rule multiqc:
    """
    Generate multiqc report for preprocessing steps
    """
    output:
        "{results_dir}/multiqc/multiqc.html",
        directory("{results_dir}/multiqc/multiqc_data"),
    log:
        "{results_dir}/logs/multiqc/multiqc.log",
    input:
        fastp=expand(
            "{results_dir}/logs/fastp/{sample}.fastp.json",
            sample=samples.keys(),
            results_dir=config["results_dir"],
        ),
    conda:
        "envs/multiqc.yml"
    envmodules:
        "bioinfo-tools",
        "MultiQC/1.12",
    resources:
        mem_mb=mem_allowed,
    params:
        tmpdir="$TMPDIR/biodivcao_multiqc",
        outdir=lambda wildcards, output: os.path.dirname(output[0]),
    shell:
        """
        mkdir -p {params.tmpdir}
        cp {input} {params.tmpdir}
        multiqc -f -o {params.outdir} -n multiqc.html {params.tmpdir} > {log} 2>&1
        rm -rf {params.tmpdir}
        """


## GENOME MAPPING ##
# Map CAO samples to the database of fish genomes. It first maps the reads,
# filters the results then blasts the mapped reads against nt. The mapping and
# blast results are filtered using taxon lineage info and a custom python script.
# Only reads that have a blast hit to a rayfinned fish (actinopterygii) or
# cartilagenous fish (chondrichthyes) are reported. Note that reads from a pair
# are considered as 1 in the final results.

# The pipeline requires the 'taxon_table.csv' to run. This file is produce when
# running the database build pipeline and contains the taxon info for each
# assembly contig.


rule bowtie2_index_genome:
    """
    Indexing of genome fasta for bowtie2
    """
    output:
        expand(
            "resources/genome_index/{{ref}}/{{ref}}.{suff}.bt2l",
            suff=["1", "2", "3", "4", "rev.1", "rev.2"],
        ),
    input:
        fasta=lambda wildcards: config["mappings"]["genomes"][wildcards.ref]["fasta"],
    log:
        "resources/logs/genome_mappings/{ref}/bowtie2/index.log",
    envmodules:
        "bioinfo-tools",
        "bowtie2/2.4.5",
    conda:
        "envs/mapping.yml"
    container:
        "https://depot.galaxyproject.org/singularity/bowtie2:2.4.5--py39hd2f7db1_3"
    threads: 20
    resources:
        runtime=48 * 60,
        mem_mb=mem_allowed,
    params:
        tmpdir="$TMPDIR/{ref}.bowtie2",
        outdir=lambda wildcards, output: os.path.dirname(output[0]),
    shell:
        """
        mkdir -p {params.tmpdir}
        cp {input} {params.tmpdir}/{wildcards.ref}

        bowtie2-build --seed 42 --large-index {params.tmpdir}/{wildcards.ref} {params.tmpdir}/{wildcards.ref} >{log} 2>&1

        mv {params.tmpdir}/*.bt2l {params.outdir}
        rm -r {params.tmpdir}
        """


rule bowtie2_map_genome:
    """
    Maps preprocessed reads against genome database
    """
    output:
        bam=temp("{results_dir}/genome_mappings/{ref}/bowtie2/{sample}/{sample}.bam"),
    input:
        idx=rules.bowtie2_index_genome.output,
        R1=rules.fastp_dedup.output.R1,
        R2=rules.fastp_dedup.output.R2,
    log:
        log="{results_dir}/logs/genome_mappings/{ref}/bowtie2/{sample}/run.log",
        maplog="{results_dir}/logs/genome_mappings/{ref}/bowtie2/{sample}/{sample}.log",
    params:
        idx=lambda wildcards, input: os.path.splitext(input.idx[0])[0].replace(".1", ""),
        tmpdir="$TMPDIR/{ref}.{sample}.bowtie2",
    conda:
        "envs/mapping.yml"
    envmodules:
        "bioinfo-tools",
        "bowtie2/2.4.5",
        "samtools/1.14",
    group:
        "group1"
    resources:
        runtime=240,
        mem_mb=mem_allowed,
    container:
        "https://depot.galaxyproject.org/singularity/bowtie2:2.4.5--py39hd2f7db1_3"
    threads: 20
    shell:
        """
        exec &>{log.log}
        mkdir -p {params.tmpdir}
        bowtie2 --seed 42 -p {threads} -x {params.idx} --very-sensitive -1 {input.R1} -2 {input.R2} 2> {log.maplog} \
            | samtools view -@ {threads} -b - | samtools sort -@ {threads} -o {params.tmpdir}/tmp.bam
        mv {params.tmpdir}/tmp.bam {output.bam}
        rm -rf {params.tmpdir}
        """


rule filter_bam:
    """
    Filter bamfile. The temporary sam file is used for parsing the results.
    """
    output:
        bam="{results_dir}/genome_mappings/{ref}/bowtie2/{sample}/{sample}.filtered.bam",
        sam="{results_dir}/genome_mappings/{ref}/bowtie2/{sample}/{sample}.filtered.sam",
    input:
        bam=rules.bowtie2_map_genome.output.bam,
        keep=lambda wildcards: config["mappings"]["genomes"][wildcards.ref]["to_include"],
    log:
        "{results_dir}/logs/genome_mappings/{ref}/bowtie2/{sample}/filter_bam.log",
    conda:
        "envs/mapping.yml"
    envmodules:
        "bioinfo-tools",
        "samtools/1.14",
    params:
        tmpdir="$TMPDIR/{ref}.{sample}",
    resources:
        runtime=120,
        mem_mb=mem_allowed,
    shell:
        """
        exec &>{log}
        mkdir -p {params.tmpdir}
        samtools view -b -L {input.keep} -h -q 40 -f 0x2 -o {params.tmpdir}/tmp.bam {input.bam}
        samtools view -L {input.keep} -q 40 -f 0x2 -o {params.tmpdir}/tmp.sam {input.bam}
        mv {params.tmpdir}/tmp.bam {output.bam}
        mv {params.tmpdir}/tmp.sam {output.sam}
        rm -rf {params.tmpdir}
        """


rule get_reads:
    """
    Extracts mapped reads from bamfile
    """
    output:
        fa="{results_dir}/genome_mappings/{ref}/bowtie2/{sample}/{sample}.reads.fa",
    input:
        bam=rules.filter_bam.output.bam,
    log:
        "{results_dir}/logs/genome_mappings/{ref}/bowtie2/{sample}/{sample}.get_reads.log",
    conda:
        "envs/mapping.yml"
    envmodules:
        "bioinfo-tools",
        "samtools/1.14",
    resources:
        runtime=30,
        mem_mb=mem_allowed,
    shell:
        """
        samtools fasta -n -o {output.fa} {input.bam} 2>{log}
        """


# Subset cleaned reads
# Checkpoint in case of contamination. For most, samples
# subsetting should result in the same number of mapped
# mapped reads


rule subset_reads:
    output:
        fa="{results_dir}/genome_mappings/{ref}/bowtie2/{sample}/subset/{sample}.reads.fa",
    input:
        fa=rules.get_reads.output.fa,
    conda:
        "envs/seqtk.yml"
    shell:
        """
        seqtk sample -s100 {input.fa} 50000 > {output.fa}
        """


# Compare read counts and flag unusual files
rule compare_counts:
    input:
        f1=rules.get_reads.output.fa,
        f2=rules.subset_reads.output.fa,
    output:
        txt="{results_dir}/genome_mappings/{ref}/bowtie2/{sample}/subset/{sample}.log.txt",
    params:
        src=srcdir("scripts/flag_unusual_files.py"),
    shell:
        """
        python {params.src} {input.f1} {input.f2} {output.txt}
        """


rule blast_reads:
    """
    Blasts reads against nt database
    """
    output:
        txt="{results_dir}/genome_mappings/{ref}/bowtie2/{sample}/blast_outs/{sample}.result.txt",
    input:
        fa=rules.subset_reads.output.fa,
    log:
        "{results_dir}/logs/blast_reads/{ref}/{sample}.log",
    params:
        db=config["blast"]["db"],
        evalue=config["blast"]["evalue"],
        max_target=config["blast"]["max_target_seqs"],
        tmp_out="$TMPDIR/{sample}.{ref}.blast_out",
    threads: config["blast"]["threads"]
    envmodules:
        "bioinfo-tools",
        "blast/2.13.0+",
        "blast_databases",
    conda:
        "envs/blast.yml"
    resources:
        runtime=24 * 60,
        mem_mb=mem_allowed,
    shell:
        """
        mkdir -p $TMPDIR
        blastn -num_threads {threads} -query {input.fa} \
        -db {params.db} -evalue {params.evalue} -max_target_seqs {params.max_target} -outfmt \
        "6 qseqid sseqid pident length \
        qstart qend sstart send evalue staxid salltitles" \
        -out {params.tmp_out} >{log} 2>&1
        mv {params.tmp_out} {output.txt}
        """


rule download_taxdump:
    """
    Downloads taxdump and generates ete3 compatible database
    """
    output:
        "resources/taxonomy/taxonomy.sqlite",
    log:
        "resources/taxonomy/download_taxdump.log",
    conda:
        "envs/ete3.yml"
    params:
        src=srcdir("scripts/download_taxdump.py"),
    shell:
        """
        python {params.src} {output[0]} 2>{log}
        """


rule parse_results:
    """
    Parses the genome mapping results and outputs counts of taxids/species
    """
    output:
        taxid="{results_dir}/genome_mappings/{ref}/counts/{sample}_taxid_counts.txt",
        species="{results_dir}/genome_mappings/{ref}/counts/{sample}_species_counts.txt",
    input:
        reads=rules.blast_reads.output[0],
        sam=rules.filter_bam.output.sam,
        tax=lambda wildcards: config["mappings"]["genomes"][wildcards.ref][
            "taxon_table"
        ],
        taxdb=ancient(rules.download_taxdump.output[0]),
    log:
        "{results_dir}/logs/blast_reads/{ref}/{sample}.parse_results.log",
    conda:
        "envs/ete3.yml"
    params:
        src=srcdir("scripts/filter_mapping_blast_results.py"),
    shell:
        """
        python {params.src} {input.reads} {input.sam} {input.tax} {output.taxid} {output.species} {input.taxdb} 2>{log}
        """


rule parse_genome_counts:
    """
    Generates raw and size adjusted counts for all samples
    """
    output:
        raw_counts="{results_dir}/genome_mappings/{ref}/summary_raw_counts.csv",
        size_adjusted="{results_dir}/genome_mappings/{ref}/summary_size_adjusted.csv",
        lib_sizes="{results_dir}/genome_mappings/{ref}/lib_sizes.csv",
    input:
        txt=expand(
            "{{results_dir}}/genome_mappings/{{ref}}/counts/{sample}_species_counts.txt",
            sample=samples_to_genomemap(config, samples),
        ),
        log=expand(
            "{{results_dir}}/logs/fastp/{sample}.fastp.log",
            sample=samples_to_genomemap(config, samples),
        ),
    log:
        "{results_dir}/logs/genome_mappings/{ref}/parse_results_w_size_factor.log",
    params:
        src=srcdir("scripts/parse_results_w_size_factor.py"),
    shell:
        """
        python {params.src} --counts {input.txt} --logs {input.log} \
            --raw_counts {output.raw_counts} --size_adjusted {output.size_adjusted} \
            --lib_sizes {output.lib_sizes} > {log} 2>&1
        """


## KrakenUniq profiling ##
rule krakenuniq_reports:
    input:
        expand(
            "{results_dir}/krakenuniq/{sample}.kreport",
            results_dir=config["results_dir"],
            sample=samples,
        ),


rule krakenuniq:
    output:
        out="{results_dir}/krakenuniq/{sample}.out",
        report="{results_dir}/krakenuniq/{sample}.kreport",
    input:
        R1=rules.fastp.output.R1,
        R2=rules.fastp.output.R2,
        db=expand(
            "{dbdir}/database.{suff}",
            dbdir=config["krakenuniq"]["db"],
            suff=["idx", "kdb", "kdb.counts"],
        ),
    log:
        ku="{results_dir}/logs/krakenuniq/{sample}.krakenuniq.log",
    envmodules:
        "bioinfo-tools",
        "KrakenUniq/1.0.1",
    conda:
        "envs/krakenuniq.yml"
    threads: 20
    resources:
        runtime=120,
        mem_mb=mem_allowed,
    group:
        "group1"
    params:
        tmpdir="$TMPDIR/{sample}.krakenuniq",
        db=lambda wildcards, input: os.path.dirname(input.db[0]),
        # here preload_size is set to 75% of the available memory
        preload_size=lambda wildcards, threads: int(
            mem_allowed(wildcards, threads) * 0.75
        ),
    shell:
        """
        mkdir -p {params.tmpdir}
        cp {input.R1} {input.R2} {params.tmpdir}
        krakenuniq --preload-size {params.preload_size}M --paired --threads {threads} \
            --output {params.tmpdir}/krakenuniq.out --report-file {params.tmpdir}/krakenreport \
            --db {params.db} {params.tmpdir}/*.fastq.gz 2>{log.ku}
        mv {params.tmpdir}/krakenuniq.out {output.out}
        mv {params.tmpdir}/krakenreport {output.report}
        rm -rf {params.tmpdir}
        """


## COI marker gene mapping ##


rule bowtie2_index:
    """
    Generates bowtie2 index database of marker gene reference fasta
    """
    output:
        idx=expand(
            "resources/{{map_name}}/bowtie2/index.{suff}.bt2l",
            suff=range(1, 5),
        ),
    input:
        ref=lambda wildcards: config["mappings"]["marker_genes"][wildcards.map_name]["fasta"]
    log:
        "resources/{map_name}/bowtie2/index.log",
    params:
        tmpdir="$TMPDIR/{map_name}.bowtie2",
        outdir=lambda wildcards, output: os.path.dirname(output[0]),
    conda:
        "envs/mapping.yml"
    resources:
        runtime=10 * 60,
        mem_mb=mem_allowed,
    threads: 4
    shell:
        """
        exec &>{log}
        mkdir -p {params.tmpdir}
        bowtie2-build --seed 42 --large-index --threads {threads} {input.ref} {params.tmpdir}/index
        mv {params.tmpdir}/index* {params.outdir}
        rm -rf {params.tmpdir}
        """


rule bowtie2:
    """
    Maps preprocessed reads against the bowtie2 index
    """
    output:
        bam="{results_dir}/mappings/{map_name}/bowtie2/{sample}.filtered.bam",
    input:
        R1=rules.fastp.output.R1,
        R2=rules.fastp.output.R2,
        idx=rules.bowtie2_index.output.idx,
    log:
        log="{results_dir}/logs/mappings/{map_name}/bowtie2/{sample}/run.log",
        maplog="{results_dir}/logs/mappings/{map_name}/bowtie2/{sample}/{sample}.log",
        coverm="{results_dir}/logs/mappings/{map_name}/bowtie2/{sample}/coverm.log",
    params:
        tmpdir="$TMPDIR/{map_name}.bowtie2.{sample}",
        ani_cutoff=lambda wildcards: mappings[wildcards.map_name]["ani_cutoff"],
        min_len=lambda wildcards: mappings[wildcards.map_name]["min_len"],
        idx=lambda wildcards, input: os.path.splitext(input.idx[0])[0].replace(".1", ""),
    conda:
        "envs/mapping.yml"
    threads: config["bowtie2"]["threads"]
    resources:
        runtime=24 * 60,
        mem_mb=mem_allowed,
    shell:
        """
        exec &>{log.log}
        mkdir -p {params.tmpdir}
        bowtie2 --seed 42 -p {threads} -x {params.idx} --very-sensitive -1 {input.R1} -2 {input.R2} 2>{log.maplog} \
            | samtools view -b - | samtools sort -o {params.tmpdir}/mapping_pairs.bam

        coverm filter -b {params.tmpdir}/mapping_pairs.bam -o {params.tmpdir}/mapping_filtered.bam --min-read-percent-identity {params.ani_cutoff} --min-read-aligned-length {params.min_len} --threads {threads} > {log.coverm} 2>&1
        mv {params.tmpdir}/mapping_filtered.bam {output.bam}
        rm -rf {params.tmpdir}
        """


rule minimap2_index:
    """
    Generates minimap2 index for marker gene reference fasta
    """
    output:
        idx="resources/{map_name}/minimap2/index",
    input:
        ref=lambda wildcards: config["mappings"]["marker_genes"][wildcards.map_name]["fasta"],
    log:
        "resources/{map_name}/minimap2/index.log",
    threads: config["minimap2"]["threads"]
    params:
        split_num=config["minimap2"]["split_num"],
        tmpdir="$TMPDIR/{map_name}.minimap2",
        outdir=lambda wildcards, output: os.path.dirname(output.idx),
    conda:
        "envs/mapping.yml"
    envmodules:
        "bioinfo-tools",
        "minimap2/2.24-r1122",
        "samtools",
    resources:
        runtime=24 * 60,
        mem_mb=mem_allowed,
    shell:
        """
        exec &>{log}
        mkdir -p {params.tmpdir}
        minimap2 -I {params.split_num}G -t {threads} -d {params.tmpdir}/index {input.ref}
        mv {params.tmpdir}/* {params.outdir}
        rm -rf {params.tmpdir}        
        """


rule minimap2:
    """
    Maps preprocessed reads against the minimap2 index
    """
    output:
        bam="{results_dir}/mappings/{map_name}/minimap2/{sample}.filtered.bam",
    input:
        R1=rules.fastp.output.R1,
        R2=rules.fastp.output.R2,
        idx=rules.minimap2_index.output.idx,
    log:
        log="{results_dir}/logs/mappings/{map_name}/minimap2/{sample}/run.log",
        maplog="{results_dir}/logs/mappings/{map_name}/minimap2/{sample}/{sample}.log",
        coverm="{results_dir}/logs/mappings/{map_name}/minimap2/{sample}/coverm.log",
    params:
        tmpdir="$TMPDIR/{map_name}.minimap2.{sample}",
        ani_cutoff=config["min_ani_cutoff"],
        min_len=config["min_align_len"],
    conda:
        "envs/mapping.yml"
    # envmodules:
    #    "bioinfo-tools",
    #    "minimap2/2.24-r1122",
    #    "samtools",
    threads: 20
    resources:
        runtime=2 * 60,
        mem_mb=mem_allowed,
    group:
        "group1"
    shell:
        """
        exec &>{log.log}
        mkdir -p {params.tmpdir}
        minimap2 -x sr --MD -a -t {threads} {input.idx} {input.R1} {input.R2} 2> {log.maplog} \
            | samtools view -b - | samtools sort -o {params.tmpdir}/mapping_pairs.bam

        coverm filter -b {params.tmpdir}/mapping_pairs.bam -o {params.tmpdir}/mapping_filtered.bam --min-read-percent-identity {params.ani_cutoff} --min-read-aligned-length {params.min_len} --threads {threads} > {log.coverm} 2>&1
        mv {params.tmpdir}/mapping_filtered.bam {output.bam}
        rm -rf {params.tmpdir}
        """


rule samtools_stats:
    """
    Generate statistics of COI mapped reads
    """
    output:
        stats="{results_dir}/mappings/{map_name}/{mapper}/{sample}.stats",
    input:
        "{results_dir}/mappings/{map_name}/{mapper}/{sample}.filtered.bam",
    log:
        "{results_dir}/logs/mappings/{map_name}/{mapper}/{sample}.filtered.samtools.stats",
    params:
        tmp="$TMPDIR/{map_name}.{mapper}.{sample}.stats",
    conda:
        "envs/mapping.yml"
    shell:
        """
        exec &>{log}
        samtools stats {input} > {params.tmp}
        mv {params.tmp} {output.stats}
        """


rule map_qc:
    """
    Generate multiqc report for samtools stats file
    """
    output:
        "{results_dir}/mappings/{map_name}/map_qc.html",
    input:
        expand(
            "{{results_dir}}/mappings/{{map_name}}/{mapper}/{sample}.stats",
            mapper=config["mappers"],
            sample=samples.keys(),
        ),
    log:
        "{results_dir}/logs/multiqc/map_qc/{map_name}.log",
    conda:
        "envs/multiqc.yml"
    params:
        input=lambda wildcards, input: "\n".join(input),
        tmpdir="$TMPDIR/{map_name}.map_qc",
        outdir=lambda wildcards, output: os.path.dirname(output[0]),
    shell:
        """
        exec &>{log}
        mkdir -p {params.tmpdir}
        echo "{params.input}" > {params.tmpdir}/files
        multiqc -f -o {params.outdir} -dd 1 -n map_qc -l {params.tmpdir}/files
        rm -rf {params.tmpdir}
        """


rule extract_reads:
    """
    Extract reads mapped against the COI database, for further assignments
    with SINTAX
    """
    output:
        fwd="{results_dir}/mappings/{map_name}/{mapper}/{sample}_fwd.fastq.gz",
        rev="{results_dir}/mappings/{map_name}/{mapper}/{sample}_rev.fastq.gz",
        unp="{results_dir}/mappings/{map_name}/{mapper}/{sample}_unp.fastq.gz",
    input:
        bam="{results_dir}/mappings/{map_name}/{mapper}/{sample}.filtered.bam",
    log:
        "{results_dir}/logs/mappings/{map_name}/{mapper}/{sample}.extract_reads.log",
    params:
        tmpdir="$TMPDIR/{map_name}.{mapper}.{sample}",
        tmp_bam="$TMPDIR/{map_name}.{mapper}.{sample}/namesorted.bam",
        outdir=lambda wildcards, output: os.path.dirname(output.fwd),
        ani_cutoff=lambda wildcards: config["mappings"]["marker_genes"][wildcards.map_name]["ani_cutoff"],
        min_len=lambda wildcards: config["mappings"]["marker_genes"][wildcards.map_name]["min_len"],
    conda:
        "envs/mapping.yml"
    resources:
        runtime=12 * 60,
        mem_mb=mem_allowed,
    threads: 4
    shell:
        """
        exec &> {log}
        mkdir -p {params.tmpdir}
        coverm filter -b {input.bam} -o {params.tmpdir}/mapping_filtered.bam --min-read-percent-identity {params.ani_cutoff} --min-read-aligned-length {params.min_len} --threads {threads}
        samtools sort -n {params.tmpdir}/mapping_filtered.bam > {params.tmp_bam}
        samtools fastq -@ {threads} {params.tmp_bam} -1 {params.tmpdir}/{wildcards.sample}_fwd.fastq -2 {params.tmpdir}/{wildcards.sample}_rev.fastq -s {params.tmpdir}/{wildcards.sample}_unp.fastq 2> {log}
        gzip {params.tmpdir}/*.fastq
        mv {params.tmpdir}/*.fastq.gz {params.outdir}
        rm -rf {params.tmpdir}
        """


rule krakenuniq_filter:
    """
    Filter away taxa marked as contaminants using krakenuniq
    """
    output:
        fastq="{results_dir}/mappings/{map_name}/{mapper}/{sample}.krakenuniq.filtered.fastq.gz",
    input:
        seq=expand(
            "{{results_dir}}/mappings/{{map_name}}/{{mapper}}/{{sample}}_{seqtype}.fastq.gz",
            seqtype=["fwd", "rev", "unp"],
        ),
        db=expand(
            "{dbdir}/database.{suff}",
            dbdir=config["krakenuniq"]["db"],
            suff=["idx", "kdb", "kdb.counts"],
        ),
    log:
        ku="{results_dir}/logs/{map_name}/{mapper}/{sample}.krakenuniq",
        extract="{results_dir}/logs/{map_name}/{mapper}/{sample}.krakenuniq.extract",
    envmodules:
        "bioinfo-tools",
        "KrakenUniq/1.0.1",
    conda:
        "envs/krakenuniq.yml"
    threads: 4
    resources:
        runtime=120,
        mem_mb=mem_allowed,
    params:
        tmpdir="$TMPDIR/{map_name}.{mapper}.{sample}.fu",
        db=lambda wildcards, input: os.path.dirname(input.db[0]),
        exclude=",".join([str(x) for x in config["krakenuniq"]["exclude"]]),
    shell:
        """
        mkdir -p {params.tmpdir}
        cat {input.seq} > {params.tmpdir}/fastq.gz
        krakenuniq --threads {threads} --output {params.tmpdir}/krakenuniq.out \
            --db {params.db} {params.tmpdir}/fastq.gz 2>{log.ku}
        krakenuniq-extract-reads -i -t {params.db}/taxDB {params.exclude} \
            {params.tmpdir}/krakenuniq.out {params.tmpdir}/fastq.gz 2>{log.extract} | gzip -c > {params.tmpdir}/filtered.fastq.gz
        mv {params.tmpdir}/filtered.fastq.gz {output} 
        """


rule run_sintax:
    """
    Runs sintax classification on the mapped and filtered reads
    """
    output:
        "{results_dir}/mappings/{map_name}/{mapper}/{sample}.sintax.tsv",
    input:
        seq=rules.krakenuniq_filter.output.fastq,
        db=config["sintax"]["db"],
    log:
        "{results_dir}/logs/mappings/{map_name}/{mapper}/{sample}.sintax.log",
    params:
        fastq="$TMPDIR/{map_name}.{mapper}.{sample}/reads.fastq.gz",
        tmpdir="$TMPDIR/{map_name}.{mapper}.{sample}",
        cutoff=config["sintax"]["cutoff"],
        out="$TMPDIR/{map_name}.{mapper}.{sample}/{sample}.sintax.tsv",
    conda:
        "envs/sintax.yml"
    resources:
        runtime=60,
        mem_mb=mem_allowed,
    threads: 2
    shell:
        """
        exec &>{log}
        mkdir -p {params.tmpdir}
        cat {input.seq} > {params.fastq}
        vsearch --sintax {params.fastq} --db {input.db} --randseed 15 --sintax_cutoff {params.cutoff} --tabbedout {params.out} --threads 1 --strand both > {log} 2>&1
        mv {params.out} {output}
        rm -rf {params.tmpdir}
        """


rule parse_sintax:
    """
    Parses the sintax output
    """
    output:
        tsv="{results_dir}/mappings/{map_name}/{mapper}/{sample}.sintax.parsed.tsv",
        krona=temp(
            "{results_dir}/mappings/{map_name}/{mapper}/{sample}.sintax.parsed.krona.txt"
        ),
    input:
        rules.run_sintax.output,
    log:
        "{results_dir}/logs/mappings/{map_name}/{mapper}/{sample}.parse_sintax.log",
    params:
        src=srcdir("scripts/parse_sintax.py"),
        cutoff=config["sintax"]["cutoff"],
        ranks=config["sintax"]["ranks"],
        replace_ranks=config["sintax"]["replace_ranks"],
    container:
        "docker://snakemake/snakemake:latest"
    shell:
        """
        python {params.src} {input} -c {params.cutoff} -k {output.krona} --ranks {params.ranks} --replace_ranks {params.replace_ranks} > {output.tsv} 2>{log}
        """


rule collate_sintax:
    """
    Collate all output from sintax parsing
    """
    output:
        "{results_dir}/mappings/counts/sintax.{map_name}.{mapper}.tsv",
    input:
        expand(
            "{{results_dir}}/mappings/{{map_name}}/{{mapper}}/{sample}.sintax.parsed.tsv",
            sample=samples.keys(),
        ),
    log:
        "{results_dir}/logs/mappings/{map_name}/{mapper}/collate_sintax.log",
    params:
        src=srcdir("scripts/collate_sintax.py"),
    shell:
        """
        python {params.src} {input} -o {output[0]} 2>{log}
        """


rule krona:
    """
    Generate krona plot for all sintax output
    """
    output:
        "{results_dir}/mappings/{map_name}/{mapper}/krona/krona.html",
    input:
        expand(
            "{{results_dir}}/mappings/{{map_name}}/{{mapper}}/{sample}.sintax.parsed.krona.txt",
            sample=samples.keys(),
        ),
    log:
        "{results_dir}/logs/mappings/{map_name}/{mapper}/krona.log",
    conda:
        "envs/krona.yml"
    params:
        input_string=krona_input_string,
    shell:
        """
        ktImportText -o {output} {params.input_string} > {log} 2>&1
        """
