import pandas as pd
import os
from snakemake.utils import validate

validate(config, "schemas/config.schema.yml")


wildcard_constraints:
    results_dir="\w+",


include: "scripts/common.py"


localrules:
    parse_results,
    gzip,
    multiqc,
    parse_sintax,
    get_reads,
    map_qc,
    download_taxdump,
    collate_counts,
    samtools_stats,
    krona,
    all,


# Read sample list
sample_df = pd.read_csv(config["sample_list"], index_col=0)
samples = sample_df.to_dict(orient="index")

# Read mapping list
mapping_df = pd.read_csv(config["mappings_list"], index_col=0)
mappings = mapping_df.to_dict(orient="index")


rule all:
    """Workflow pseudo rule"""
    input:
        all_output,


rule genome_map:
    input:
        expand(
            "{results_dir}/genome_mappings/{ref}/bowtie2/{sample}/{sample}.reads.fa",
            results_dir=config["results_dir"],
            sample=samples.keys(),
            ref=config["mappings"]["genomes"].keys(),
        ),


rule genome_count:
    input:
        expand(
            "{results_dir}/genome_mappings/{ref}/collated_counts/{t}_counts.txt",
            results_dir=config["results_dir"],
            sample=samples.keys(),
            t=["taxid", "species"],
            ref=config["mappings"]["genomes"].keys(),
        ),


rule coi_map:
    input:
        expand(
            "{results_dir}/mappings/{map_name}/{mapper}/{sample}_{r}.fastq.gz",
            results_dir=config["results_dir"],
            map_name=mappings.keys(),
            mapper=config["mappers"],
            sample=samples.keys(),
            r=["fwd", "rev", "unp"],
        ),


rule sintax:
    input:
        expand(
            "{results_dir}/mappings/{map_name}/{mapper}/krona/krona.html",
            results_dir=config["results_dir"],
            map_name=mappings.keys(),
            mapper=config["mappers"],
        ),


rule gzip:
    output:
        "{f}.fastq.gz",
    input:
        "{f}.fastq",
    log:
        "{f}.gzip.log",
    container:
        "docker://snakemake/snakemake:latest"
    shell:
        """
        gzip -c {input} > {output} 2>{log}
        """


rule fastp:
    output:
        R1=temp("{results_dir}/fastp/{sample}.fastp.R1.fastq.gz"),
        R2=temp("{results_dir}/fastp/{sample}.fastp.R2.fastq.gz"),
    log:
        log="{results_dir}/logs/fastp/{sample}.fastp.log",
        json="{results_dir}/logs/fastp/{sample}.fastp.json",
    input:
        R1=lambda wildcards: sorted(
            [x for x in samples[wildcards.sample]["fwd_libs"].split(";")]
        ),
        R2=lambda wildcards: sorted(
            [x for x in samples[wildcards.sample]["rev_libs"].split(";")]
        ),
    envmodules:
        "bioinfo-tools",
        "fastp/0.23.2",
    conda:
        "envs/fastp.yml"
    group:
        "group1"
    resources:
        runtime=60,
        mem_mb=mem_allowed,
    threads: 20
    params:
        tmpR1="$TMPDIR/{sample}.R1.fastq.gz",
        tmpR2="$TMPDIR/{sample}.R2.fastq.gz",
        outR1="$TMPDIR/{sample}.fastp.R1.fastq.gz",
        outR2="$TMPDIR/{sample}.fastp.R2.fastq.gz",
        complexity_threshold=config["fastp"]["complexity_threshold"],
    shell:
        """
        cat {input.R1} > {params.tmpR1}
        cat {input.R2} > {params.tmpR2}
        fastp --thread {threads} -y -Y {params.complexity_threshold} \
            -i {params.tmpR1} -I {params.tmpR2} -o {params.outR1} \
            -O {params.outR2} -j {log.json} > {log.log} 2>&1
        mv {params.outR1} {output.R1}
        mv {params.outR2} {output.R2}
        rm {params.tmpR1} {params.tmpR2}
        """


rule multiqc:
    output:
        "{results_dir}/multiqc/multiqc.html",
        directory("{results_dir}/multiqc/multiqc_data"),
    log:
        "{results_dir}/logs/multiqc/multiqc.log",
    input:
        fastp=expand(
            "{results_dir}/logs/fastp/{sample}.fastp.json",
            sample=samples.keys(),
            results_dir=config["results_dir"],
        ),
    conda:
        "envs/multiqc.yml"
    envmodules:
        "bioinfo-tools",
        "MultiQC/1.12",
    resources:
        mem_mb=mem_allowed,
    params:
        tmpdir="$TMPDIR/biodivcao_multiqc",
        outdir=lambda wildcards, output: os.path.dirname(output[0]),
    shell:
        """
        mkdir -p {params.tmpdir}
        cp {input} {params.tmpdir}
        multiqc -f -o {params.outdir} -n multiqc.html {params.tmpdir} > {log} 2>&1
        rm -rf {params.tmpdir}
        """


## GENOME MAPPING ##
# Map CAO samples to the database of fish genomes. It first maps the reads,
# filters the results then blasts the mapped reads against nt. The mapping and
# blast results are filtered using taxon lineage info and a custom python script.
# Only reads that have a blast hit to a rayfinned fish (actinopterygii) or
# cartilagenous fish (chondrichthyes) are reported. Note that reads from a pair
# are considered as 1 in the final results.

# The pipeline requires the 'taxon_table.csv' to run. This file is produce when
# running the database build pipeline and contains the taxon info for each
# assembly contig.


rule bowtie2_index_genome:
    output:
        expand(
            "resources/genome_index/{{ref}}/{{ref}}.{suff}.bt2l",
            suff=["1", "2", "3", "4", "rev.1", "rev.2"],
        ),
    input:
        fasta=lambda wildcards: config["mappings"]["genomes"][wildcards.ref]["fasta"],
    log:
        "resources/logs/genome_mappings/{ref}/bowtie2/index.log",
    envmodules:
        "bioinfo-tools",
        "bowtie2/2.4.5",
    conda:
        "envs/mapping.yml"
    container:
        "https://depot.galaxyproject.org/singularity/bowtie2:2.4.5--py39hd2f7db1_3"
    threads: 20
    resources:
        runtime=48 * 60,
        mem_mb=mem_allowed,
    params:
        tmpdir="$TMPDIR/{ref}.bowtie2",
        outdir=lambda wildcards, output: os.path.dirname(output[0]),
    shell:
        """
        mkdir -p {params.tmpdir}
        cp {input} {params.tmpdir}/{wildcards.ref}

        bowtie2-build --seed 42 --large-index {params.tmpdir}/{wildcards.ref} {params.tmpdir}/{wildcards.ref} >{log} 2>&1

        mv {params.tmpdir}/*.bt2l {params.outdir}
        rm -r {params.tmpdir}
        """


rule bowtie2_map_genome:
    output:
        bam=temp("{results_dir}/genome_mappings/{ref}/bowtie2/{sample}/{sample}.bam"),
    input:
        idx=rules.bowtie2_index_genome.output,
        R1=rules.fastp.output.R1,
        R2=rules.fastp.output.R2,
    log:
        log="{results_dir}/logs/genome_mappings/{ref}/bowtie2/{sample}/run.log",
        maplog="{results_dir}/logs/genome_mappings/{ref}/bowtie2/{sample}/{sample}.log",
    params:
        idx=lambda wildcards, input: os.path.splitext(input.idx[0])[0].replace(".1", ""),
        tmpdir="$TMPDIR/{ref}.{sample}.bowtie2",
    conda:
        "envs/mapping.yml"
    envmodules:
        "bioinfo-tools",
        "bowtie2/2.4.5",
        "samtools/1.14",
    group:
        "group1"
    resources:
        runtime=4 * 60,
        mem_mb=mem_allowed,
    container:
        "https://depot.galaxyproject.org/singularity/bowtie2:2.4.5--py39hd2f7db1_3"
    threads: 20
    shell:
        """
        exec &>{log.log}
        mkdir -p {params.tmpdir}
        bowtie2 --seed 42 -p {threads} -x {params.idx} --very-sensitive -1 {input.R1} -2 {input.R2} 2> {log.maplog} \
            | samtools view -b - | samtools sort -o {params.tmpdir}/tmp.bam
        mv {params.tmpdir}/tmp.bam {output.bam}
        rm -rf {params.tmpdir}
        """


# Filter bamfile. The temporary sam file is used for parsing the results.
rule filter_bam:
    input:
        bam=rules.bowtie2_map_genome.output.bam,
    output:
        bam="{results_dir}/genome_mappings/{ref}/bowtie2/{sample}/{sample}.filtered.bam",
        sam=temp(
            "{results_dir}/genome_mappings/{ref}/bowtie2/{sample}/{sample}.filtered.sam"
        ),
    log:
        "{results_dir}/logs/genome_mappings/{ref}/bowtie2/{sample}/filter_bam.log",
    conda:
        "envs/mapping.yml"
    envmodules:
        "bioinfo-tools",
        "samtools/1.14",
    params:
        tmpdir="$TMPDIR/{ref}.{sample}",
    resources:
        runtime=2 * 60,
        mem_mb=mem_allowed,
    shell:
        """
        exec &>{log}
        mkdir -p {params.tmpdir}
        samtools view -b -h -q 40 -f 0x2 -o {params.tmpdir}/tmp.bam {input.bam}
        samtools view -h -q 40 -f 0x2 -o {params.tmpdir}/tmp.sam {input.bam}
        mv {params.tmpdir}/tmp.bam {output.bam}
        mv {params.tmpdir}/tmp.sam {output.sam}
        rm -rf {params.tmpdir}
        """


# Get mapped reads:
rule get_reads:
    input:
        bam=rules.filter_bam.output.bam,
    output:
        fa="{results_dir}/genome_mappings/{ref}/bowtie2/{sample}/{sample}.reads.fa",
    log:
        "{results_dir}/logs/genome_mappings/{ref}/bowtie2/{sample}/{sample}.get_reads.log",
    conda:
        "envs/mapping.yml"
    envmodules:
        "bioinfo-tools",
        "samtools/1.14",
    resources:
        runtime=30,
    shell:
        """
        samtools fasta -n -o {output.fa} {input.bam} 2>{log}
        """


# Blast reads against nt
rule blast_reads:
    input:
        fa=rules.get_reads.output[0],
    output:
        txt="{results_dir}/genome_mappings/{ref}/bowtie2/{sample}/blast_outs/{sample}.result.txt",
    log:
        "{results_dir}/logs/blast_reads/{ref}/{sample}.log",
    params:
        db=config["blast"]["db"],
        evalue=config["blast"]["evalue"],
        max_target=config["blast"]["max_target_seqs"],
    threads: 10
    envmodules:
        "bioinfo-tools",
        "blast/2.9.0+",
        "blast_databases",
    conda:
        "envs/blast.yml"
    resources:
        runtime=24 * 60,
        mem_mb=mem_allowed,
    shell:
        """
        blastn -num_threads {threads} -query {input.fa} \
        -db {params.db} -evalue {params.evalue} -max_target_seqs {params.max_target} -outfmt \
        "6 qseqid sseqid pident length mismatch gapopen \
        qstart qend sstart send evalue bitscore staxid salltitles" \
        -out {output.txt} >{log} 2>&1
        """


# Filter mapping result using blast results and
# taxon lineage info. Report counts per species
# and counts per taxon for each sample.


rule download_taxdump:
    output:
        "resources/taxonomy/taxonomy.sqlite",
    log:
        "resources/taxonomy/download_taxdump.log",
    conda:
        "envs/ete3.yml"
    params:
        src=srcdir("scripts/download_taxdump.py"),
    shell:
        """
        python {params.src} 2>{log}
        """


rule parse_results:
    input:
        reads=rules.blast_reads.output[0],
        sam=rules.filter_bam.output.sam,
        tax=lambda wildcards: config["mappings"]["genomes"][wildcards.ref][
            "taxon_table"
        ],
        taxdb=rules.download_taxdump.output[0],
    output:
        taxid="{results_dir}/genome_mappings/{ref}/counts/{sample}_taxid_counts.txt",
        species="{results_dir}/genome_mappings/{ref}/counts/{sample}_species_counts.txt",
    log:
        "{results_dir}/logs/blast_reads/{ref}/{sample}.parse_results.log",
    conda:
        "envs/ete3.yml"
    params:
        src=srcdir("scripts/filter_aln_and_blast_results.py"),
    shell:
        """
        python {params.src} {input.reads} {input.sam} {input.tax} {output.taxid} {output.species} {input.taxdb} 2>{log}
        """


rule collate_counts:
    output:
        txt="{results_dir}/genome_mappings/{ref}/collated_counts/{t}_counts.txt",
    input:
        expand(
            "{{results_dir}}/genome_mappings/{{ref}}/counts/{sample}_{{t}}_counts.txt",
            sample=samples.keys(),
        ),
    log:
        "{results_dir}/logs/collate_counts/{ref}.{t}_counts.txt",
    params:
        src=srcdir("scripts/collate_counts.py"),
    shell:
        """
        python {params.src} {input} {output.txt} --strip {wildcards.t}_counts.txt 2>{log}
        """


rule bowtie2_index:
    output:
        idx=expand(
            "resources/{{map_name}}/bowtie2/index.{suff}.bt2l",
            suff=range(1, 5),
        ),
    input:
        ref=lambda wildcards: mappings[wildcards.map_name]["reference"],
    log:
        "resources/{map_name}/bowtie2/index.log",
    params:
        tmpdir="$TMPDIR/{map_name}.bowtie2",
        outdir=lambda wildcards, output: os.path.dirname(output[0]),
    conda:
        "envs/mapping.yml"
    resources:
        runtime=10 * 60,
        mem_mb=mem_allowed,
    threads: 4
    shell:
        """
        exec &>{log}
        mkdir -p {params.tmpdir}
        bowtie2-build --seed 42 --large-index --threads {threads} {input.ref} {params.tmpdir}/index
        mv {params.tmpdir}/index* {params.outdir}
        rm -rf {params.tmpdir}
        """


rule bowtie2:
    output:
        bam="{results_dir}/mappings/{map_name}/bowtie2/{sample}.filtered.bam",
    input:
        R1=rules.fastp.output.R1,
        R2=rules.fastp.output.R2,
        idx=rules.bowtie2_index.output.idx,
    log:
        log="{results_dir}/logs/mappings/{map_name}/bowtie2/{sample}/run.log",
        maplog="{results_dir}/logs/mappings/{map_name}/bowtie2/{sample}/{sample}.log",
        coverm="{results_dir}/logs/mappings/{map_name}/bowtie2/{sample}/coverm.log",
    params:
        tmpdir="$TMPDIR/{map_name}.bowtie2.{sample}",
        ani_cutoff=lambda wildcards: mappings[wildcards.map_name]["ani_cutoff"],
        min_len=lambda wildcards: mappings[wildcards.map_name]["min_len"],
        idx=lambda wildcards, input: os.path.splitext(input.idx[0])[0].replace(".1", ""),
    conda:
        "envs/mapping.yml"
    threads: config["bowtie2"]["threads"]
    resources:
        runtime=24 * 60,
        mem_mb=mem_allowed,
    shell:
        """
        exec &>{log.log}
        mkdir -p {params.tmpdir}
        bowtie2 --seed 42 -p {threads} -x {params.idx} --very-sensitive -1 {input.R1} -2 {input.R2} 2>{log.maplog} \
            | samtools view -b - | samtools sort -o {params.tmpdir}/mapping_pairs.bam

        coverm filter -b {params.tmpdir}/mapping_pairs.bam -o {params.tmpdir}/mapping_filtered.bam --min-read-percent-identity {params.ani_cutoff} --min-read-aligned-length {params.min_len} --threads {threads} > {log.coverm} 2>&1
        mv {params.tmpdir}/mapping_filtered.bam {output.bam}
        rm -rf {params.tmpdir}
        """


rule minimap2_index:
    output:
        idx="resources/{map_name}/minimap2/index",
    input:
        ref=lambda wildcards: mappings[wildcards.map_name]["reference"],
    log:
        "resources/{map_name}/minimap2/index.log",
    threads: config["minimap2"]["threads"]
    params:
        split_num=config["minimap2"]["split_num"],
        tmpdir="$TMPDIR/{map_name}.minimap2",
        outdir=lambda wildcards, output: os.path.dirname(output.idx),
    conda:
        "envs/mapping.yml"
    envmodules:
        "bioinfo-tools",
        "minimap2/2.24-r1122",
    resources:
        runtime=24 * 60,
        mem_mb=mem_allowed,
    shell:
        """
        exec &>{log}
        mkdir -p {params.tmpdir}
        minimap2 -I {params.split_num}G -t {threads} -d {params.tmpdir}/index {input.ref}
        mv {params.tmpdir}/* {params.outdir}
        rm -rf {params.tmpdir}        
        """


rule minimap2:
    output:
        bam="{results_dir}/mappings/{map_name}/minimap2/{sample}.filtered.bam",
    input:
        R1=rules.fastp.output.R1,
        R2=rules.fastp.output.R2,
        idx=rules.minimap2_index.output.idx,
    log:
        log="{results_dir}/logs/mappings/{map_name}/minimap2/{sample}/run.log",
        maplog="{results_dir}/logs/mappings/{map_name}/minimap2/{sample}/{sample}.log",
        coverm="{results_dir}/logs/mappings/{map_name}/minimap2/{sample}/coverm.log",
    params:
        tmpdir="$TMPDIR/{map_name}.minimap2.{sample}",
        ani_cutoff=config["min_ani_cutoff"],
        min_len=config["min_align_len"],
    conda:
        "envs/mapping.yml"
    envmodules:
        "bioinfo-tools",
        "minimap2/2.24-r1122",
    threads: 20
    resources:
        runtime=2 * 60,
        mem_mb=mem_allowed,
    group:
        "group1"
    shell:
        """
        exec &>{log.log}
        mkdir -p {params.tmpdir}
        minimap2 -x sr --MD -a -t {threads} {input.idx} {input.R1} {input.R2} 2> {log.maplog} \
            | samtools view -b - | samtools sort -o {params.tmpdir}/mapping_pairs.bam

        coverm filter -b {params.tmpdir}/mapping_pairs.bam -o {params.tmpdir}/mapping_filtered.bam --min-read-percent-identity {params.ani_cutoff} --min-read-aligned-length {params.min_len} --threads {threads} > {log.coverm} 2>&1
        mv {params.tmpdir}/mapping_filtered.bam {output.bam}
        rm -rf {params.tmpdir}
        """


rule samtools_stats:
    output:
        stats="{results_dir}/mappings/{map_name}/{mapper}/{sample}.stats",
    input:
        "{results_dir}/mappings/{map_name}/{mapper}/{sample}.filtered.bam",
    log:
        "{results_dir}/logs/mappings/{map_name}/{mapper}/{sample}.filtered.samtools.stats",
    params:
        tmp="$TMPDIR/{map_name}.{mapper}.{sample}.stats",
    conda:
        "envs/mapping.yml"
    shell:
        """
        exec &>{log}
        samtools stats {input} > {params.tmp}
        mv {params.tmp} {output.stats}
        """


rule map_qc:
    output:
        "{results_dir}/mappings/{map_name}/map_qc.html",
    input:
        expand(
            "{{results_dir}}/mappings/{{map_name}}/{mapper}/{sample}.stats",
            mapper=config["mappers"],
            sample=samples.keys(),
        ),
    log:
        "{results_dir}/logs/multiqc/map_qc/{map_name}.log",
    conda:
        "envs/multiqc.yml"
    params:
        input=lambda wildcards, input: "\n".join(input),
        tmpdir="$TMPDIR/{map_name}.map_qc",
        outdir=lambda wildcards, output: os.path.dirname(output[0]),
    shell:
        """
        exec &>{log}
        mkdir -p {params.tmpdir}
        echo "{params.input}" > {params.tmpdir}/files
        multiqc -f -o {params.outdir} -dd 1 -n map_qc -l {params.tmpdir}/files
        rm -rf {params.tmpdir}
        """


rule extract_reads:
    output:
        fwd="{results_dir}/mappings/{map_name}/{mapper}/{sample}_fwd.fastq.gz",
        rev="{results_dir}/mappings/{map_name}/{mapper}/{sample}_rev.fastq.gz",
        unp="{results_dir}/mappings/{map_name}/{mapper}/{sample}_unp.fastq.gz",
    input:
        bam="{results_dir}/mappings/{map_name}/{mapper}/{sample}.filtered.bam",
    log:
        "{results_dir}/logs/mappings/{map_name}/{mapper}/{sample}.extract_reads.log",
    params:
        tmpdir="$TMPDIR/{map_name}.{mapper}.{sample}",
        tmp_bam="$TMPDIR/{map_name}.{mapper}.{sample}/namesorted.bam",
        outdir=lambda wildcards, output: os.path.dirname(output.fwd),
        ani_cutoff=lambda wildcards: mappings[wildcards.map_name]["ani_cutoff"],
        min_len=lambda wildcards: mappings[wildcards.map_name]["min_len"],
    conda:
        "envs/mapping.yml"
    resources:
        runtime=12 * 60,
        mem_mb=mem_allowed,
    threads: 4
    shell:
        """
        exec &> {log}
        mkdir -p {params.tmpdir}
        coverm filter -b {input.bam} -o {params.tmpdir}/mapping_filtered.bam --min-read-percent-identity {params.ani_cutoff} --min-read-aligned-length {params.min_len} --threads {threads}
        samtools sort -n {params.tmpdir}/mapping_filtered.bam > {params.tmp_bam}
        samtools fastq -@ {threads} {params.tmp_bam} -1 {params.tmpdir}/{wildcards.sample}_fwd.fastq -2 {params.tmpdir}/{wildcards.sample}_rev.fastq -s {params.tmpdir}/{wildcards.sample}_unp.fastq 2> {log}
        gzip {params.tmpdir}/*.fastq
        mv {params.tmpdir}/*.fastq.gz {params.outdir}
        rm -rf {params.tmpdir}
        """


rule run_sintax:
    output:
        "{results_dir}/mappings/{map_name}/{mapper}/{sample}.sintax.tsv",
    input:
        seq=expand(
            "{{results_dir}}/mappings/{{map_name}}/{{mapper}}/{{sample}}_{seqtype}.fastq.gz",
            seqtype=["fwd", "rev", "unp"],
        ),
        db=config["sintax"]["db"],
    log:
        "{results_dir}/logs/mappings/{map_name}/{mapper}/{sample}.sintax.log",
    params:
        fastq="$TMPDIR/{map_name}.{mapper}.{sample}/reads.fastq.gz",
        tmpdir="$TMPDIR/{map_name}.{mapper}.{sample}",
        cutoff=config["sintax"]["cutoff"],
        out="$TMPDIR/{map_name}.{mapper}.{sample}/{sample}.sintax.tsv",
    conda:
        "envs/sintax.yml"
    resources:
        runtime=60,
        mem_mb=mem_allowed,
    threads: 2
    shell:
        """
        exec &>{log}
        mkdir -p {params.tmpdir}
        cat {input.seq} > {params.fastq}
        vsearch --sintax {params.fastq} --db {input.db} --randseed 15 --sintax_cutoff {params.cutoff} --tabbedout {params.out} --threads 1 --strand both > {log} 2>&1
        mv {params.out} {output}
        rm -rf {params.tmpdir}
        """


rule parse_sintax:
    output:
        tsv="{results_dir}/mappings/{map_name}/{mapper}/{sample}.sintax.parsed.tsv",
        krona=temp(
            "{results_dir}/mappings/{map_name}/{mapper}/{sample}.sintax.parsed.krona.txt"
        ),
    input:
        rules.run_sintax.output,
    log:
        "{results_dir}/logs/mappings/{map_name}/{mapper}/{sample}.parse_sintax.log",
    params:
        src=srcdir("scripts/parse_sintax.py"),
        cutoff=config["sintax"]["cutoff"],
    container:
        "docker://snakemake/snakemake:latest"
    shell:
        """
        python {params.src} -c {params.cutoff} -k {output.krona} {input} > {output.tsv} 2>{log}
        """


rule krona:
    output:
        "{results_dir}/mappings/{map_name}/{mapper}/krona/krona.html",
    input:
        expand(
            "{{results_dir}}/mappings/{{map_name}}/{{mapper}}/{sample}.sintax.parsed.krona.txt",
            sample=samples.keys(),
        ),
    log:
        "{results_dir}/logs/mappings/{map_name}/{mapper}/krona.log",
    conda:
        "envs/krona.yml"
    params:
        input_string=krona_input_string,
    shell:
        """
        ktImportText -o {output} {params.input_string} > {log} 2>&1
        """
